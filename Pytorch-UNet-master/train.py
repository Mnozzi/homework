import argparse
import logging
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from pathlib import Path
from torch import optim
from torch.utils.data import DataLoader, SubsetRandomSampler
from tqdm import tqdm
import numpy as np

import wandb
from evaluate import evaluate
from unet import UNet
from utils.data_loading import BasicDataset


dir_img = Path('/kaggle/working/homework/Pytorch-UNet-master/data/train/img')
dir_mask = Path('/kaggle/working/homework/Pytorch-UNet-master/data/train/mask')
dir_checkpoint = Path('/kaggle/working/homework/Pytorch-UNet-master/checkpoints')


def train_model(
        fold,
        model,
        train_loader,
        val_loader,
        device,
        epochs: int = 5,
        batch_size: int = 1,
        learning_rate: float = 1e-5,
        val_percent: float = 0.1,
        save_checkpoint: bool = True,
        img_scale: float = 0.5,
        amp: bool = False,
        weight_decay: float = 1e-8,
        momentum: float = 0.999,
        gradient_clipping: float = 1.0,
        early_stop_patience: int = 5
):
    # (Initialize logging)
    experiment = wandb.init(project='U-Net', name=f'fold_{fold}', mode="disabled")
    experiment.config.update(
        dict(epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,
             val_percent=val_percent, save_checkpoint=save_checkpoint, img_scale=img_scale, amp=amp)
    )

    n_val = int(len(val_loader) * batch_size)
    n_train = int(len(train_loader) * batch_size)

    logging.info(f'''Starting training:
        Epochs:          {epochs}
        Batch size:      {batch_size}
        Learning rate:   {learning_rate}
        Training size:   {n_train}
        Validation size: {n_val}
        Checkpoints:     {save_checkpoint}
        Device:          {device.type}
        Images scaling:  {img_scale}
        Mixed Precision: {amp}
    ''')

    # 4. Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP
    optimizer = optim.RMSprop(model.parameters(),
                              lr=learning_rate, weight_decay=weight_decay, momentum=momentum)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5)  # goal: maximize Dice score

    '''For device==cuda version'''
    # grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)
    '''For device==cuda version'''

    criterion = nn.CrossEntropyLoss()
    global_step = 0
    best_val_score = -np.inf  # å‡è®¾ç›‘æ§æŒ‡æ ‡æ˜¯Diceåˆ†æ•°ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
    epochs_no_improve = 0
    best_model_path = '/kaggle/working/homework/Pytorch-UNet-master/earlymodel'  # ä¿å­˜æœ€ä½³æ¨¡å‹è·¯å¾„
    # # 5. Begin training
    for epoch in range(1, epochs + 1):
        model.train()
        epoch_loss = 0
        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar:
            for batch in train_loader:
                images, true_masks = batch['image'], batch['mask']

                # æ•°æ®æ ¡éªŒ
                assert images.shape[1] == model.n_channels, \
                    f'è¾“å…¥å›¾åƒçš„é€šé“æ•° ({images.shape[1]}) ä¸æ¨¡å‹å®šä¹‰ ({model.n_channels}) ä¸åŒ¹é…'

                # æ•°æ®æ¬ç§»åˆ°è®¾å¤‡
                images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)
                true_masks = true_masks.to(device=device, dtype=torch.long)

                # æ··åˆç²¾åº¦è®­ç»ƒ
                with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):
                    masks_pred = model(images)
                    loss = criterion(masks_pred, true_masks)

                # åå‘ä¼ æ’­
                optimizer.zero_grad(set_to_none=True)
                loss.backward()
                optimizer.step()

                # æ›´æ–°è¿›åº¦æ¡å’Œæ—¥å¿—
                pbar.update(images.shape[0])
                global_step += 1
                epoch_loss += loss.item()
                experiment.log({'train loss': loss.item(), 'step': global_step, 'epoch': epoch})
                pbar.set_postfix(**{'loss (batch)': loss.item()})

                # --- éªŒè¯é˜¶æ®µ (æ¯ä¸ªepochç»“æŸåæ‰§è¡Œä¸€æ¬¡) ---
                val_score, val_ce = evaluate(model, val_loader, device, amp)
                scheduler.step(val_score)  # ä»…æ ¹æ®Diceåˆ†æ•°è°ƒæ•´å­¦ä¹ ç‡

                # --- æ—©åœé€»è¾‘ ---
                if val_score > best_val_score:
                    best_val_score = val_score
                    epochs_no_improve = 0
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    best_model_path = str(dir_checkpoint / f'best_model_fold{fold}.pth')
                    torch.save(model.state_dict(), best_model_path)
                    logging.info(f'ğŸ”¥ Epoch {epoch}: æœ€ä½³æ¨¡å‹å·²ä¿å­˜, Diceåˆ†æ•°={val_score:.4f}')
                else:
                    epochs_no_improve += 1
                    logging.info(f'â³ Epoch {epoch}: {epochs_no_improve}/{early_stop_patience} æ¬¡æœªæå‡')

                    # è§¦å‘æ—©åœ
                    if epochs_no_improve >= early_stop_patience:
                        logging.info(f'ğŸ›‘ Epoch {epoch}: æ—©åœè§¦å‘!')
                        break  # ç»ˆæ­¢è®­ç»ƒå¾ªç¯

                # --- å¸¸è§„æ¨¡å‹ä¿å­˜ (å¯é€‰) ---
                if save_checkpoint:
                    checkpoint_path = str(dir_checkpoint / f'checkpoint_epoch{epoch}.pth')
                    torch.save(model.state_dict(), checkpoint_path)
                    logging.info(f'ğŸ“¦ Epoch {epoch}: å¸¸è§„æ£€æŸ¥ç‚¹å·²ä¿å­˜')

                # --- è®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æ¨¡å‹ ---
            if best_model_path:
                model.load_state_dict(torch.load(best_model_path))
                logging.info(f'ğŸ¯ æœ€ä½³æ¨¡å‹å·²åŠ è½½: {best_model_path}')
            else:
                logging.warning('âš ï¸ æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹')

            return model

def get_args():
    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks')
    parser.add_argument('--epochs', '-e', metavar='E', type=int, default=10, help='Number of epochs')
    parser.add_argument('--batch-size', '-b', dest='batch_size', metavar='B', type=int, default=2, help='Batch size')
    parser.add_argument('--learning-rate', '-l', metavar='LR', type=float, default=1e-4,
                        help='Learning rate', dest='lr')
    parser.add_argument('--load', '-f', type=str, default=False, help='Load model from a .pth file')
    parser.add_argument('--scale', '-s', type=float, default=0.5, help='Downscaling factor of the images')
    parser.add_argument('--validation', '-v', dest='val', type=float, default=25.0,
                        help='Percent of the data that is used as validation (0-100)')
    parser.add_argument('--amp', action='store_true', default=False, help='Use mixed precision')
    parser.add_argument('--bilinear', action='store_true', default=False, help='Use bilinear upsampling')
    parser.add_argument('--classes', '-c', type=int, default=2, help='Number of classes')
    parser.add_argument('--early-stop', '-es', type=int, default=5,
                        help='Early stopping patience (epochs with no improvement)')

    return parser.parse_args()


if __name__ == '__main__':
    args = get_args()

    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logging.info(f'Using device {device}')

    # Change here to adapt to your data
    # n_channels=3 for RGB images
    # n_classes is the number of probabilities you want to get per pixel
    model = UNet(n_channels=3, n_classes=args.classes, bilinear=args.bilinear)
    model = model.to(memory_format=torch.channels_last)

    logging.info(f'Network:\n'
                 f'\t{model.n_channels} input channels\n'
                 f'\t{model.n_classes} output channels (classes)\n'
                 f'\t{"Bilinear" if model.bilinear else "Transposed conv"} upscaling')

    if args.load:
        state_dict = torch.load(args.load, map_location=device)
        del state_dict['mask_values']
        model.load_state_dict(state_dict)
        logging.info(f'Model loaded from {args.load}')
    # pdb.set_trace()
    model.to(device=device)

    # 1. Create dataset
    dataset = BasicDataset(dir_img, dir_mask, args.scale)

    # # 2. Split into train / validation partitions
    # TODO: ä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯å®Œæˆæ¨¡å‹çš„è®­ç»ƒå’ŒéªŒè¯
    # å®šä¹‰kå€¼å’ŒæŠ˜æ•°
    k = 5
    num_samples = len(dataset)
    #fold_size = num_samples // k
    # ç”Ÿæˆéšæœºç´¢å¼•
    indices = np.arange(num_samples)
    np.random.shuffle(indices)

    # TODO: åˆ›å»ºæ¯ä¸ªæŠ˜å¯¹åº”çš„Datasetæ ·æœ¬ç´¢å¼•
    fold_indices = np.array_split(indices, k)

    # ä½¿ç”¨æ¯ä¸ªæŠ˜è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯
    for fold in range(k):
        # æ¯ä¸ª Fold æ–°å»ºæ¨¡å‹
        model = UNet(n_channels=3, n_classes=args.classes, bilinear=args.bilinear)
        model.to(device=device)

        if args.load:  # åŠ è½½é¢„è®­ç»ƒæƒé‡
            state_dict = torch.load(args.load, map_location=device)
            del state_dict['mask_values']
            model.load_state_dict(state_dict)

        # åˆ’åˆ†æ•°æ®é›†
        val_idx = fold_indices[fold].tolist()
        train_idx = np.concatenate([f for i, f in enumerate(fold_indices) if i != fold]).tolist()
        train_sampler = SubsetRandomSampler(train_idx)
        val_sampler = SubsetRandomSampler(val_idx)

        # åˆ›å»º DataLoader
        train_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler,
                                  num_workers=4, pin_memory=True)
        val_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=val_sampler,
                                num_workers=4, pin_memory=True)

        # è®­ç»ƒæ¨¡å‹ï¼ˆä¼˜åŒ–å™¨åœ¨ train_model å†…åˆå§‹åŒ–ï¼‰
        trained_model = train_model(
            fold,
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.lr,
            early_stop_patience=args.early_stop,  # ä¼ é€’æ—©åœå‚æ•°
            device=device,
            img_scale=args.scale,
            val_percent=args.val / 100,
            amp=args.amp
        )
